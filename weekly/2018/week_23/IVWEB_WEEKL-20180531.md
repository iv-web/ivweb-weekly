## 文章索引
1、 <a href="#1文章-腾讯云devops技术揭秘新时代运维重器tencent-hub最佳实践" >文章： 腾讯云DevOps技术揭秘：新时代运维重器Tencent Hub最佳实践</a><br/>
2、 <a href="#2文章-区块链通往自主主权身份的道路" >文章： 区块链：通往自主主权身份的道路</a><br/>
3、 <a href="#3文章-看板使用指南从三列看板到灵活的布局设计" >文章： 看板使用指南：从三列看板到灵活的布局设计</a><br/>
4、 <a href="#4视频访谈-黄冠辉ai会话平台的现状挑战与未来" >视频访谈： 黄冠辉：AI会话平台的现状、挑战与未来</a><br/>
5、 <a href="#5文章-以腾讯云iot-suite为例-谈谈边缘计算在物联网的实践与实现" >文章： 以腾讯云IoT Suite为例 谈谈边缘计算在物联网的实践与实现</a><br/>
6、 <a href="#6区块链技术落地思考" >区块链技术落地思考</a><br/>
7、 <a href="#7文章-ambari接管线上hadoop游戏数据集群实践" >文章： Ambari接管线上Hadoop游戏数据集群实践</a><br/>
8、 <a href="#8亚马逊发布用于以太坊和hyperledger-fabric的区块链模板" >亚马逊发布用于以太坊和Hyperledger Fabric的区块链模板</a><br/>
9、 <a href="#9arcore-12让用户共享ar世界" >ARCore 1.2让用户共享AR世界</a><br/>
10、 <a href="#10快乐文化如何培育高绩效" >快乐文化如何培育高绩效</a><br/>
11、 <a href="#11物联网技术周报第-138-期:-使用树莓派构建一个婴儿监视器" >物联网技术周报第 138 期: 使用树莓派构建一个婴儿监视器</a><br/>
12、 <a href="#12building-a-central-logging-service-in-house" >Building A Central Logging Service In-House</a><br/><h1 id="#title_0" >1、文章： 腾讯云DevOps技术揭秘：新时代运维重器Tencent Hub最佳实践</h1>
张蝉
[http://www.infoq.com/cn/articles/tencent-cloud-devops?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global](http://www.infoq.com/cn/articles/tencent-cloud-devops?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global)
<img src="https://res.infoq.com/articles/tencent-cloud-devops/zh/smallimage/cloud-logo-1527651703399.jpeg"/><p>在上周腾讯“云+未来”峰会的开发者专场上，来自腾讯云的几位开发者和大家分享了DevOps，数据库，微服务框架，边缘计算，机器学习等话题的实践，为大家带来了一场技术盛宴。在这里和大家一起分享腾讯云PaaS产品总监邹辉带来的Tencent Hub最佳实践，一窥腾讯云DevOps产品的设计理念和思考。</p> <i>By 张蝉</i>
---------------
<h1 id="#title_1" >2、文章： 区块链：通往自主主权身份的道路</h1>
本体, 季宙栋
[http://www.infoq.com/cn/articles/blockchain-way-to-autonomous-sovereignty?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global](http://www.infoq.com/cn/articles/blockchain-way-to-autonomous-sovereignty?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global)
<img src="https://res.infoq.com/articles/blockchain-way-to-autonomous-sovereignty/zh/smallimage/agil-series-1508534878617-1527697281398.jpeg"/><p>区块链与自主主权身份赋予了我们使用数字身份在保持个人隐私的同时建立信任的能力。</p> <i>By 本体</i>
---------------
<h1 id="#title_2" >3、文章： 看板使用指南：从三列看板到灵活的布局设计</h1>
Alex Novkov
[http://www.infoq.com/cn/articles/kanban-step-guide?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global](http://www.infoq.com/cn/articles/kanban-step-guide?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global)
<img src="https://res.infoq.com/articles/kanban-step-guide/zh/headerimage/GettyImages-641906720-1526337182068.jpg"/><p>虽然实现看板看起来相当简单，但只有那些愿意尝试自己的工作流程并且将测试结果反映到实际步骤中的人才能够充分利用该方法。这篇文章将介绍实现看板的最常见阶段，并在逐渐熟练掌握该方法时学习如何提高工作流可视化和控制程度。</p> <i>By Alex Novkov</i> <i> Translated by 无明</i>
---------------
<h1 id="#title_3" >4、视频访谈： 黄冠辉：AI会话平台的现状、挑战与未来</h1>
黄冠辉
[http://www.infoq.com/cn/interviews/interview-with-huangguanhui-talk-ai-session-platform?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global](http://www.infoq.com/cn/interviews/interview-with-huangguanhui-talk-ai-session-platform?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global)
<img src="https://res.infoq.com/interviews/interview-with-huangguanhui-talk-ai-session-platform/zh/mediumimage/huangguanghui270-1527518205059.jpg"/><p>我们在QCon 全球软件开发大会，2018北京站的现场，很荣幸地采访到了来自Oracle甲骨文亚太区的移动化、数字化解决方案的技术顾问黄冠辉先生，请他来谈谈Oracle会话AI平台的发展现在及他本人对AI会话平台前景的看法。</p> <i>By 黄冠辉</i>
---------------
<h1 id="#title_4" >5、文章： 以腾讯云IoT Suite为例 谈谈边缘计算在物联网的实践与实现</h1>
江柳
[http://www.infoq.com/cn/articles/TencentCloud-IoT-SDK?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global](http://www.infoq.com/cn/articles/TencentCloud-IoT-SDK?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global)
<img src="https://res.infoq.com/articles/TencentCloud-IoT-SDK/zh/smallimage/12-1527659991895.jpg"/><p></p> <i>By 江柳</i>
---------------
<h1 id="#title_5" >6、区块链技术落地思考</h1>
BCCon
[http://www.infoq.com/cn/news/2018/05/blockchain-practice?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global](http://www.infoq.com/cn/news/2018/05/blockchain-practice?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global)
<img src="http://www.infoq.com/styles/i/logo_bigger.jpg"/><p>如果现在要我们谈区块链技术落地，我们可以谈什么？</p> <i>By BCCon</i>
---------------
<h1 id="#title_6" >7、文章： Ambari接管线上Hadoop游戏数据集群实践</h1>
陈燃
[http://www.infoq.com/cn/articles/ambari-hadoop-game-data-cluster-practice?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global](http://www.infoq.com/cn/articles/ambari-hadoop-game-data-cluster-practice?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global)
<img src="https://res.infoq.com/articles/ambari-hadoop-game-data-cluster-practice/zh/smallimage/cloud-1527613378121.jpg"/><p>本文首先将以简要介绍游戏公司生产环境中Hadoop集群现状、Ambari的关键技术，然后将重点阐述我们Ambari管理监控线上Hadoop集群的技术方案，介绍线上接管过程中碰见的问题和解决方式。</p> <i>By 陈燃</i>
---------------
<h1 id="#title_7" >8、亚马逊发布用于以太坊和Hyperledger Fabric的区块链模板</h1>
Alex Giamas
[http://www.infoq.com/cn/news/2018/05/AWS-Blockchain-Ethereum-Fabric?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global](http://www.infoq.com/cn/news/2018/05/AWS-Blockchain-Ethereum-Fabric?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global)
<img src="http://www.infoq.com/styles/i/logo_bigger.jpg"/><p>亚马逊最近宣布为以太坊和Hyperledger Fabric引入区块链模板。 AWS区块链模板用于帮助开发人员快速搭建区块链基础设施，让他们专注于构建应用程序，不必处理底层基础设施细节，以及如何构建、维护和保护好应用程序。</p> <i>By Alex Giamas</i> <i> Translated by 无明</i>
---------------
<h1 id="#title_8" >9、ARCore 1.2让用户共享AR世界</h1>
Sergio De Simone
[http://www.infoq.com/cn/news/2018/05/arcore-1.2-anchor-clouds?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global](http://www.infoq.com/cn/news/2018/05/arcore-1.2-anchor-clouds?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global)
<img src="http://www.infoq.com/styles/i/logo_bigger.jpg"/><p>在最近举行的I/O 2018大会上，谷歌宣布了其增强现实框架ARCore的1.2版本，该版本通过云锚、垂直面检测和SceneForms（让不使用OpenGL创建3D App成为可能）提供协作式AR体验。</p> <i>By Sergio De Simone</i> <i> Translated by 谢丽</i>
---------------
<h1 id="#title_9" >10、快乐文化如何培育高绩效</h1>
Rafiq Gemmail
[http://www.infoq.com/cn/news/2018/05/happy-teams-are-high-performers?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global](http://www.infoq.com/cn/news/2018/05/happy-teams-are-high-performers?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global)
<img src="http://www.infoq.com/styles/i/logo_bigger.jpg"/><p>在今年二月的DOXLON大会上，英国独立电视台（ITV）通用平台负责人Tom Clark做演讲介绍了他是如何根据Dan Pink关于自主权、掌控权和目的性的原则，在ITV创建快乐并有活力的团队。近期在Agile Uprising播客的采访中，Andy Flemming针对一个组织如何通过有意识地创造关注员工个体的学习、成长和幸福的文化进而在商业和战略利益上获益的问题，介绍了他对此的一些观察。</p> <i>By Rafiq Gemmail</i> <i> Translated by 盖磊</i>
---------------
<h1 id="#title_10" >11、物联网技术周报第 138 期: 使用树莓派构建一个婴儿监视器</h1>
黄峰达
[http://www.infoq.com/cn/news/2018/05/iot-weekly-138?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global](http://www.infoq.com/cn/news/2018/05/iot-weekly-138?utm_campaign=infoq_content&utm_source=infoq&utm_medium=feed&utm_term=global)
<img src="http://www.infoq.com/styles/i/logo_bigger.jpg"/><p>使用树莓派构建一个婴儿监视器；在 Raspberry Pi 的 Docker 中构建、运行和测试 .NET Core 和 ASP.NET Core 2.1；AliOS Things SMP 系统及其在 ESP 32 上实现示例；腾讯与中国联通发布物联网 SIM 卡 主打用户身份鉴权；欧盟 GDPR 即将生效，物联网企业遵从 GDPR 合规性的 10 个步骤；Android Things 1.0 支持更多硬件，并新增配置界面；从 2018 年STM32峰会看 ARM 核 MCU 发展趋势</p> <i>By 黄峰达</i>
---------------
<h1 id="#title_11" >12、Building A Central Logging Service In-House</h1>
Akhil Labudubariki
[https://www.smashingmagazine.com/2018/05/building-central-logging-service/](https://www.smashingmagazine.com/2018/05/building-central-logging-service/)
<html>
            <head>
              <meta charset="utf-8">
              <link rel="canonical" href="https://www.smashingmagazine.com/2018/05/building-central-logging-service/" />
              <title>Building A Central Logging Service In-House </title>
            </head>
            <body>
              <article>
                <header>
                  <h1>Building A Central Logging Service In-House </h1>
                  
                    
                    <address>Akhil Labudubariki</address>
                  
                  <time datetime="2018-05-30T13:30:22&#43;02:00" class="op-published">2018-05-30T13:30:22+02:00</time>
                  <time datetime="2018-05-30T21:37:55&#43;00:00" class="op-modified">2018-05-30T21:37:55+00:00</time>
                </header>
                

<p>We all know how important debugging is for improving application performance and features. BrowserStack runs one million sessions a day on a highly distributed application stack! Each involves several moving parts, as a client’s single session can span multiple components across several geographic regions.</p>

<p>Without the right framework and tools, the debugging process can be a nightmare. In our case, we needed a way to collect events happening during different stages of each process in order to get an in-depth understanding of everything taking place during a session. With our infrastructure, solving this problem became complicated as each component might have multiple events from their lifecycle of processing a request.</p>

<p>That’s why we developed our own in-house Central Logging Service tool (CLS) to record all important events logged during a session. These events help our developers identify conditions where something goes wrong in a session and helps keep track of certain key product metrics.</p>

<p>Debugging data ranges from simple things like API response latency to monitoring a user&rsquo;s network health. In this article, we share our story of building our CLS tool which collects 70G of relevant chronological data per day from 100+ components reliably, at scale and with two M3.large EC2 instances.</p>



  <aside class="product-panel product-panel__tilted product-panel--book" data-audience="non-subscriber">
    <div class="container product-panel--book__container">
      <div class="panel__description panel__description--book">
    <p>Getting the process <em>just</em> right ain't an easy task. That's why we've set up <strong>'this-is-how-I-work'-sessions</strong> — with smart cookies sharing what works really well for them. A part of the , of course.</p>
      <a href="http://smashed.by/casestudypanelmembership" class="btn btn--green btn--large">
        Explore features&nbsp;→
      </a>
      </div>
      <div class="panel__image panel__image--book">
        <a href="http://smashed.by/casestudypanelmembership" class="books__book__image">
        <div class="books__book__img">
          <img src="https://www.smashingmagazine.com/images/smashing-cat/cat-scubadiving-panel.svg" alt="Smashing TV, with live sessions for professional designers and developers." width="310" height="400">
        </div>
      </a>
      </div>
    </div>
  </aside>





<div class="c-garfield-the-cat">


<h3 id="the-decision-to-build-in-house">The Decision To Build In-House</h3>

<p>First, let’s consider why we built our CLS tool in-house rather than used an existing solution. Each of our sessions sends 15 events on average, from multiple components to the service - translating into approximately 15 million total events per day.</p>

<p>Our service needed the ability to store all this data. We sought a complete solution to support event storing, sending and querying across events. As we considered third-party solutions such as Amplitude and Keen, our evaluation metrics included cost, performance in handling high parallel requests and ease of adoption. Unfortunately, we could not find a fit that met all our requirements within budget - although benefits would have included saving time and minimizing alerts. While it would take additional effort, we decided to develop an in-house solution ourselves.</p>











<figure class="article__image break-out">
	<a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8644db7c-42d9-4d38-b4e5-a8ee12f678a2/1-building-central-logging-service.jpg">
		<img
			srcset="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8644db7c-42d9-4d38-b4e5-a8ee12f678a2/1-building-central-logging-service.jpg 400w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8644db7c-42d9-4d38-b4e5-a8ee12f678a2/1-building-central-logging-service.jpg 800w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8644db7c-42d9-4d38-b4e5-a8ee12f678a2/1-building-central-logging-service.jpg 1200w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8644db7c-42d9-4d38-b4e5-a8ee12f678a2/1-building-central-logging-service.jpg 1600w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8644db7c-42d9-4d38-b4e5-a8ee12f678a2/1-building-central-logging-service.jpg 2000w"
			src="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/8644db7c-42d9-4d38-b4e5-a8ee12f678a2/1-building-central-logging-service.jpg"
			sizes="100vw"
			alt="Building in-house"
		/>
	</a>

	
		<figcaption class="op-vertical-bottom">
			One of the biggest issues with building In-house is the amount of resources that we need to spend to maintain it. (Image credit: )
		</figcaption>
	
</figure>


<h3 id="technical-details">Technical Details</h3>

<p>In terms of architecting for our component, we outlined the following basic requirements:</p>

<ul>
<li><strong>Client Performance</strong><br />
Does not impact the performance of the client/component sending the events.</li>
<li><strong>Scale</strong><br />
Able to handle a high number of requests in parallel.</li>
<li><strong>Service performance</strong><br />
Quick to process all events being sent to it.</li>
<li><strong>Insight into data</strong><br />
Each event logged needs to have some meta information to be able to uniquely identify the component or user, account or message and give more information to help the developer debug faster.</li>
<li><strong>Queryable interface</strong><br />
Developers can query all events for a particular session, helping to debug a particular session, build component health reports, or generate meaningful performance statistics of our systems.</li>
<li><strong>Faster and easier adoption</strong><br />
Easy integration with an existing or new component without burdening teams and taking up their resources.</li>
<li><strong>Low maintenance</strong><br />
We are a small engineering team, so we sought a solution to minimize alerts!</li>
</ul>

<h3 id="building-our-cls-solution">Building Our CLS Solution</h3>

<h4 id="decision-1-choosing-an-interface-to-expose">Decision 1: Choosing An Interface To Expose</h4>

<p>In developing CLS, we obviously didn’t want to lose any of our data, but we didn’t want component performance to take a hit either. Not to mention the additional factor of preventing existing components from becoming more complicated, since it would delay overall adoption and release. In determining our interface, we considered the following choices:</p>

<ol>
<li>Storing events in local Redis in each component, as a background processor pushes it to CLS. However, this requires a change in all components, along with an introduction of Redis for components which didn’t already contain it.</li>
<li>A Publisher - Subscriber model, where Redis is closer to the CLS.  As everyone publishes events, again we have the factor of components running across the globe. During the time of high-traffic, this would delay components. Further, this write could intermittently jump up to five seconds (due to the internet alone).</li>
<li>Sending events over UDP, which offers a lesser impact on application performance. In this case data would be sent and forgotten, however, the disadvantage here would be data loss.</li>
</ol>

<p>Interestingly, our data loss over UDP was less than 0.1 percent, which was an acceptable amount for us to consider building such a service. We were able to convince all teams that this amount of loss was worth the performance, and went ahead to leverage a UDP interface that listened to all events being sent.</p>

<p>While one result was a smaller impact on an application’s performance, we did face an issue as UDP traffic was not allowed from all networks, mostly from our users’ - causing us in some cases to receive no data at all. As a workaround, we supported logging events using HTTP requests. All events coming from the user’s side would be sent via HTTP, whereas all events being recorded from our components would be via UDP.</p>

<h4 id="decision-2-tech-stack-language-framework-storage">Decision 2: Tech Stack (Language, Framework &amp; Storage)</h4>

<p>We are a Ruby shop. However, we were uncertain if Ruby would be a better choice for our particular problem. Our service would have to handle a lot of incoming requests, as well as process a lot of writes. With the Global Interpreter lock, achieving multithreading or concurrency would be difficult in Ruby (please don&rsquo;t take offense - we love Ruby!).  So we needed a solution that would help us achieve this kind of concurrency.</p>

<p>We were also keen to evaluate a new language in our tech stack, and this project seemed perfect for experimenting with new things. That’s when we decided to give Golang a shot since it offered inbuilt support for concurrency and lightweight threads and go-routines. Each logged data point resembles a key-value pair where ‘key’ is the event and ‘value’ serves as its associated value.</p>

<p>But having a simple key and value is not enough to retrieve a session related data - there is more metadata to it. To address this, we decided any event needing to be logged would have a session ID along with its key and value. We also added extra fields like timestamp, user ID and the component logging the data, so that it became more easy to fetch and analyze data.</p>

<p>Now that we decided on our payload structure, we had to choose our datastore. We considered Elastic Search, but we also wanted to support update requests for keys. This would trigger the entire document to be re-indexed, which might affect the performance of our writes. MongoDB made more sense as a datastore since it would be easier to query all events based on any of the data fields that would be added. This was easy!</p>


  <div id="sponsors-article" data-impression="true" class="c-promo-box c-promo-box--ad sponsors hidden" data-audience="non-subscriber" data-remove="true"></div>



<h4 id="decision-3-db-size-is-huge-and-query-and-archiving-sucks">Decision 3: DB Size Is Huge And Query And Archiving Sucks!</h4>

<p>In order to cut maintenance, our service would have to handle as many events as possible. Given the rate that BrowserStack releases features and products, we were certain the number of our events would increase at higher rates over time, meaning our service would have to continue to perform well. As space increases, reads and writes take more time – which could be a huge hit on the service&rsquo;s performance.</p>

<p>The first solution we explored was moving logs from a certain period away from the database (in our case, we decided on 15 days). To do this, we created a different database for each day, allowing us to find logs older than a particular period without having to scan all written documents. Now we continually remove databases older than 15 days from Mongo, while of course keeping backups just in case.</p>

<p>The only leftover piece was a developer interface to query session-related data. Honestly, this was the easiest problem to solve. We provide an HTTP interface, where people can query for session related events in the corresponding database in the MongoDB, for any data having a particular session ID.</p>

<h3 id="architecture">Architecture</h3>

<p>Let&rsquo;s talk about the internal components of the service, considering the following points:</p>

<ol>
<li>As previously discussed, we needed two interfaces - one listening over UDP and another listening over HTTP.  So we built two servers, again one for each interface, to listen for events. As soon as an event arrives, we parse it to check whether it has the required fields - these are session ID, key, and value. If it does not, the data is dropped. Otherwise, the data is passed over a Go channel to another goroutine, whose sole responsibility is to write to the MongoDB.</li>
<li>A possible concern here is writing to the MongoDB. If writes to the MongoDB are slower than the rate data is received, this creates a bottleneck. This, in turn, starves other incoming events and means dropped data. The server, therefore, should be fast in processing incoming logs and be ready to process ones upcoming. To address the issue, we split the server into two parts: the first receives all events and queues them up for the second, which processes and writes them into the MongoDB.</li>
<li>For queuing we chose Redis. By dividing the entire component into these two pieces we reduced the server’s workload, giving it room to handle more logs.</li>
<li>We wrote a small service using Sinatra server to handle all the work of querying MongoDB with given parameters. It returns an HTML/JSON response to developers when they need information on a particular session.</li>
</ol>

<p>All these processes happily run on a single <em>m3.large</em> instance.</p>











<figure class="article__image break-out">
	<a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f0b437a6-e58f-4177-bba2-91e80045d2a8/building-central-logging-service-cls-v1.png">
		<img
			srcset="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f0b437a6-e58f-4177-bba2-91e80045d2a8/building-central-logging-service-cls-v1.png 400w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f0b437a6-e58f-4177-bba2-91e80045d2a8/building-central-logging-service-cls-v1.png 800w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f0b437a6-e58f-4177-bba2-91e80045d2a8/building-central-logging-service-cls-v1.png 1200w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f0b437a6-e58f-4177-bba2-91e80045d2a8/building-central-logging-service-cls-v1.png 1600w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f0b437a6-e58f-4177-bba2-91e80045d2a8/building-central-logging-service-cls-v1.png 2000w"
			src="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/f0b437a6-e58f-4177-bba2-91e80045d2a8/building-central-logging-service-cls-v1.png"
			sizes="100vw"
			alt="CLS v1"
		/>
	</a>

	
		<figcaption class="op-vertical-bottom">
			CLS v1: A representation of the system’s first architecture. All the components are running on one single machine.
		</figcaption>
	
</figure>


<h3 id="feature-requests">Feature Requests</h3>

<p>As our CLS tool saw more use over time, it needed more features. Below, we discuss these and how they were added.</p>

<h4 id="missing-metadata">Missing Metadata</h4>

<p>Gradually as the number of components in BrowserStack increases, we’ve demanded more from CLS. For example, we needed the ability to log events from components lacking a session ID. Otherwise obtaining one would burden our infrastructure, in the form of affecting application performance and incurring traffic on our main servers.</p>

<p>We addressed this by enabling event logging using other keys, such as terminal and user IDs. Now whenever a session is created or updated, CLS is informed with the session ID, as well as the respective user and terminal IDs. It stores a map that can be retrieved by the process of writing to MongoDB. Whenever an event that contains either the user or terminal ID is retrieved, the session ID is added.</p>

<h4 id="handle-spamming-code-issues-in-other-components">Handle Spamming (Code Issues In Other Components)</h4>

<p>CLS also faced the usual difficulties with handling spam events. We often found deploys in components that generated a huge volume of requests sent to CLS. Other logs would suffer in the process, as the server became too busy to process these and important logs were dropped.</p>

<p>For the most part, most of the data being logged were via HTTP requests. To control them we enable rate limiting on nginx (using the limit_req_zone module), which blocks requests from any IP we found hitting requests more than a certain number in a small amount of time. Of course, we do leverage health reports on all blocked IPs and inform the responsible teams.</p>

<h4 id="scale-v2">Scale v2</h4>

<p>As our sessions per day increased, data being logged to CLS was also increasing. This affected the queries our developers were running daily, and soon the bottleneck we had was with the machine itself. Our setup consisted of two core machines running all of the above components, along with a bunch of scripts to query Mongo and keep track of key metrics for each product. Over time, data on the machine had increased heavily and scripts began to take a lot of CPU time. Even after trying to optimizing Mongo queries, we always came back to the same issues.</p>

<p>To solve this, we added another machine for running health report scripts and the interface to query these sessions. The process involved booting a new machine and setting up a slave of the Mongo running on the main machine. This has helped reduce the CPU spikes we saw every day caused by these scripts.</p>











<figure class="article__image break-out">
	<a href="https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4fff0cf3-29bc-4167-80a5-eba45e7715a5/building-central-logging-service-cls-v2.png">
		<img
			srcset="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4fff0cf3-29bc-4167-80a5-eba45e7715a5/building-central-logging-service-cls-v2.png 400w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_800/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4fff0cf3-29bc-4167-80a5-eba45e7715a5/building-central-logging-service-cls-v2.png 800w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1200/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4fff0cf3-29bc-4167-80a5-eba45e7715a5/building-central-logging-service-cls-v2.png 1200w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_1600/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4fff0cf3-29bc-4167-80a5-eba45e7715a5/building-central-logging-service-cls-v2.png 1600w,
			        https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_2000/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4fff0cf3-29bc-4167-80a5-eba45e7715a5/building-central-logging-service-cls-v2.png 2000w"
			src="https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_auto/w_400/https://cloud.netlifyusercontent.com/assets/344dbf88-fdf9-42bb-adb4-46f01eedd629/4fff0cf3-29bc-4167-80a5-eba45e7715a5/building-central-logging-service-cls-v2.png"
			sizes="100vw"
			alt="CLS v2"
		/>
	</a>

	
		<figcaption class="op-vertical-bottom">
			CLS v2: A representation of the current system’s architecture. Logs are written to the master machine and they are synced on the slave machine. Developer’s queries run on the slave machine.
		</figcaption>
	
</figure>


<h3 id="conclusion">Conclusion</h3>

<p>Building a service for a task as simple as data logging can get complicated, as the amount of data increases. This article discusses the solutions we explored, along with challenges faced while solving this problem. We experimented with Golang to see how well it would fit with our ecosystem, and so far we have been satisfied. Our choice to create an internal service rather than paying for an external one has been wonderfully cost-efficient. We also didn’t have to scale our setup to another machine until much later - when the volume of our sessions increased. Of course, our choices in developing CLS were completely based on our requirements and priorities.</p>

<p>Today CLS handles up to 15 million events every day, constituting up to 70 GB of data. This data is being used to help us solve any issues our customers face during any session. We also use this data for other purposes. Given the insights each session’s data provides on different products and internal components, we’ve begun leveraging this data to keep track of each product. This is achieved by extracting the key metrics for all the important components.</p>

<p>All in all, we’ve seen great success in building our own CLS tool. If it makes sense for you, I recommend you consider doing the same!</p>




<div class="signature">
  <img src="https://www.smashingmagazine.com/images/logo/logo--red.png" alt="Smashing Editorial">
  <span>(rb, ra, il)</span>
</div>


</div>




  <div id="sponsors-article-end" data-impression="true" class="c-promo-box c-promo-box--ad c-promo-box--wide sponsors hidden" data-audience="non-subscriber" data-remove="true"></div>



              </article>
            </body>
          </html>
---------------